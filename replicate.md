### Instructions for Implementing the `/chat` Endpoint

**Set Up the FastAPI Application:**
   - Create a FastAPI application with a basic structure to handle incoming requests.
   - Define the `/chat` endpoint to receive a message, pass it to the Llama model, and return the response.

**Load the Llama Model:**
   - Load the Llama model from the specified repository and filename. Use the `Llama.from_pretrained()` method to load the model with the correct parameters.
   - Use the **Tinyllama-2-1b-miniguanaco** model with the filename `tinyllama-2-1b-miniguanaco.Q2_K.gguf`, available from the `TheBloke/Tinyllama-2-1b-miniguanaco-GGUF` repository.

**Implement the Response Function:**
   - Create a function (`get_llm_response`) that constructs a prompt using the user’s message and system instructions.
   - Call the Llama model with the constructed prompt, set parameters `temperature` to 0 and `max_tokens` to be greater than 64, and parse the response to extract the assistant’s text.

**Create the `/chat` Endpoint:**
   - Implement the `/chat` endpoint using the `@app.post()` decorator. This endpoint should accept the message model, call the response function, and return the text generated by the Llama model.

**Build a Docker Image:**
   - Write a Dockerfile that installs the required dependencies, including `llama-cpp-python==0.2.89`.
   - Specify the platform as `amd64`

**Deploy the Service to Azure Kubernetes Service (AKS):**
   - Use tools like Helm or Kubernetes CLI to deploy the Docker image to the AKS cluster, ensuring it runs on a compatible `amd64` platform.
